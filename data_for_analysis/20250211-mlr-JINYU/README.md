# 机器学习特征筛选与超参数调优集成框架

本项目使用mlr3框架实现了特征筛选与超参数调优在嵌套交叉验证中的集成方法，用于构建高性能机器学习模型。

## 概述

在机器学习模型开发中，特征筛选和超参数调优是两个关键步骤。传统方法通常是分开进行这两个步骤：先进行特征筛选，然后使用筛选后的特征进行超参数调优。这种方法可能会导致数据泄露和过拟合问题，因为特征筛选过程已经"看到"了全部数据。

本项目使用嵌套交叉验证方法，将特征筛选视为模型训练过程的一部分，与超参数调优同时进行，从而避免上述问题，提高模型的泛化能力。

## 方法原理

### 嵌套交叉验证

嵌套交叉验证由两层交叉验证组成：
- **外层交叉验证**：用于评估模型性能
- **内层交叉验证**：用于特征筛选和超参数调优

这种设计确保了模型评估的无偏性，因为在每个外层折叠中，模型从头开始训练（包括特征筛选和超参数调优）。

### 特征筛选与超参数调优的集成

本项目将特征筛选作为学习管道的一部分，并同时调优两部分的参数：
1. 特征筛选参数（如筛选比例）
2. 学习算法参数（如正则化强度、树深度等）

通过这种集成方法，我们可以同时找到最佳特征子集和最佳超参数组合。

## 实现细节

### 数据预处理

在进行特征筛选和超参数调优前，我们先进行数据预处理：
- 缺失值处理：连续变量使用直方图插补，分类变量使用众数插补
- 去除常量特征
- 可选的标准化/归一化

### 特征筛选方法

项目支持多种特征筛选方法：

#### 1. Boruta算法

Boruta是一种基于随机森林的特征选择方法，其核心思想是比较真实特征与随机生成的影子特征的重要性，从而确定哪些特征是真正重要的。

**主要参数**：
- `maxRuns`：算法最大迭代次数(默认100)
- `pValue`：显著性水平(默认0.01)
- `mcAdj`：是否进行多重比较校正(默认TRUE)
- `doTrace`：输出调试信息的级别(默认0)
- `num.threads`：使用的线程数(默认1)

**在mlr3 PipeOp中使用时的选择标准参数(必选其一)**：
- `frac`：保留重要性排名前X%的特征(例如0.8表示保留80%最重要的特征)
- `nfeat`：保留固定数量的特征
- `cutoff`：根据重要性分数的截断值选择特征
- `permuted`：选择所有重要性大于随机特征的特征

注意：在mlr3中，Boruta算法有两种用法：
1. 作为`flt("boruta")`直接用于特征重要性排序，使用`maxRuns`等参数配置算法本身
2. 在`po("filter")`管道中使用时，除了算法参数外，还必须指定一个特征选择标准(如`frac`)来确定保留哪些特征

#### 2. 递归特征消除(RFE)

逐步移除最不重要的特征，直到达到预设的特征数量。

#### 3. 基于互信息的特征选择

使用最大互信息最小化联合互信息(JMIM)方法选择特征。

### 机器学习算法

本框架集成了多种机器学习算法：
1. **Elastic Net**：结合L1和L2正则化的线性模型
2. **XGBoost**：高效的梯度提升树算法
3. **随机森林**：集成多个决策树的算法
4. **支持向量机(SVM)**：适用于线性和非线性分类的经典算法

### 超参数调优

使用随机搜索或Hyperband算法进行超参数调优，结合多种终止条件：
- 性能停滞：连续几次迭代性能提升低于阈值时停止
- 评估次数：达到最大评估次数时停止
- 运行时间：超过最大运行时间时停止
- 目标性能：达到目标性能时停止

## 使用指南

### 必备依赖

```r
# 核心包
library(mlr3verse)
library(mlr3learners)
library(mlr3tuning)
library(mlr3extralearners)
library(mlr3filters)  # 特征筛选包
library(paradox)
library(future)

# 特征选择算法依赖包
library(Boruta)       # Boruta特征选择算法
library(rpart)        # 用于基于树的特征选择

# 可视化和数据处理
library(ggplot2)
library(tidyverse)
library(data.table)

# 机器学习算法
library(ranger)
library(glmnet)
library(xgboost)
library(e1071)
```

### 工作流程

1. **数据准备**：加载数据，定义任务(分类/回归)
2. **预处理**：处理缺失值，移除常量特征等
3. **定义重采样策略**：设置内外层交叉验证的折数
4. **创建特征筛选方法**：选择合适的特征筛选算法
   ```r
   # Boruta特征选择器示例
   boruta_filter <- flt("boruta",
       perc = 0.8,      # 保留80%重要特征
       max_runs = 20,   # 最大迭代次数
       ntree = 100,     # 森林中的树数量
       doTrace = 0      # 关闭调试输出
   )
   ```
5. **定义学习算法及参数空间**：选择机器学习算法和需要调优的参数
   ```r
   # 包含特征筛选参数的参数空间示例
   ps_rf <- ps(
       classif.ranger.num.trees = p_int(lower = 500, upper = 1000),
       boruta.perc = p_dbl(lower = 0.5, upper = 0.9) # Boruta保留特征比例作为超参数
   )
   ```
6. **创建自动调优器**：集成特征筛选和参数调优
7. **执行基准测试**：在外层交叉验证中评估模型性能
8. **选择最佳模型**：根据性能指标选择最佳模型
9. **分析特征重要性**：了解哪些特征对模型贡献最大

## 结果评估

模型评估使用多种性能指标：
- **AUC (Area Under ROC Curve)**：衡量分类器区分能力
- **准确率 (Accuracy)**：正确分类的样本比例
- **分类错误率 (Classification Error)**：错误分类的样本比例

## 注意事项

1. 嵌套交叉验证计算成本较高，对于大型数据集或复杂模型可能需要较长时间
2. 特征筛选方法的选择应根据具体问题和数据特点来定
3. 将特征筛选参数(如`boruta.perc`)作为超参数进行调优可能会增加计算负担，在资源有限时可以固定该参数
4. Boruta算法在mlr3filters中的实现与其他包可能有所不同，请注意参数设置

## 参考资料

- [mlr3 Book](https://mlr3book.mlr-org.com/)
- [Feature Selection with mlr3filters](https://mlr3filters.mlr-org.com/)
- [Nested Resampling](https://mlr3book.mlr-org.com/chapters/chapter4/hyperparameter_optimization.html#nested-resampling)
- [Boruta算法原理](https://www.jstatsoft.org/article/view/v036i11)