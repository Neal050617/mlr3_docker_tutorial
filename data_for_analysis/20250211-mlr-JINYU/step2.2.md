# 集成特征选择与进阶模型构建流程

## 第一阶段：集成特征选择（Ensemble Feature Selection, EFS）

### 数据准备
- 使用第一步预处理后的数据
- 将数据转换为分类任务，目标变量为`feature13`
- 识别并区分分类变量和连续变量
- 执行基础数据预处理（缺失值插补、常量特征移除等）

### 集成特征选择设计
1. **基础学习器配置**
   - XGBoost
   - 随机森林（Random Forest）
   - 支持向量机（SVM）
   - LightGBM

2. **特征选择策略**
   - 采用递归特征消除（RFE）方法
   - 设置目标特征数量为20
   - 特征保留比例为0.85

3. **回调函数设置**
   - XGBoost：one-se规则 + 内部调优
   - 随机森林：one-se规则
   - SVM：one-se规则 + SVM-RFE
   - LightGBM：one-se规则

### 特征选择评估
- 使用子采样验证：
  - 在`tNGS`脚本中，使用50次重复的子采样验证（比例0.8）
  - 在`clinic`脚本中，使用100次重复的子采样验证（比例0.8）
- 内部使用5折交叉验证
- 评估指标：分类错误率（CE）和准确率（Accuracy）
- 生成多个评估可视化：
  1. 性能分布图
  2. 特征数量分布图
  3. 帕累托前沿分析
  4. 特征稳定性分析

### 特征重要性分析
- 基于集合投票的特征排序（SAV方法）
- 确定最优特征子集大小（基于knee point方法）
- 可视化特征重要性得分

## 第二阶段：模型优化与评估

### 模型构建
使用筛选后的特征子集构建五种分类模型：

1. **弹性网络（Elastic Net）**
   - 参数优化范围：
     - lambda：1e-4 到 1（对数尺度）
     - alpha：0 到 1
     - 标准化选项
     - 收敛阈值：1e-7 到 1e-4

2. **XGBoost**
   - 参数优化范围：
     - 学习率：0.01 到 0.3
     - 树深度：1 到 10
     - 迭代次数：50 到 500
     - 采样比例：0.5 到 1

3. **随机森林**
   - 参数优化范围：
     - 树数量：500 到 1000
     - 最大深度：3 到 15
     - 最小节点大小：1 到 10
     - 特征采样数：1 到 20

4. **支持向量机**
   - 参数优化范围：
     - 代价参数：10^(-2) 到 10^2
     - 核函数：多项式或径向基
     - 多项式度数：1 到 3

5. **LightGBM**
   - 参数优化范围：
     - 学习率：0.01 到 0.3
     - 迭代次数：50 到 500
     - 树深度：3 到 12
     - 叶子节点数：10 到 255
     - 特征和样本采样比例：0.5 到 1.0

### 超参数优化设计
- 采用嵌套交叉验证：
  - 内层：5折交叉验证
  - 外层：5折交叉验证
- 使用随机搜索策略
- 终止条件：
  - 性能停滞（10次迭代内提升<0.005）
  - 最大评估次数：100
  - 最大运行时间：1小时
  - 目标性能：AUC达到0.85

### 模型评估
- 主要评估指标：
  - AUC（曲线下面积）
  - 准确率（Accuracy）
  - 分类错误率（CE）
- 生成评估可视化：
  - 模型性能对比图
  - ROC曲线对比
  - 各模型特征重要性分析

### 结果保存与分析
- 保存基准测试结果
- 记录各模型的训练和测试性能
- 保存特征重要性分析结果
- 对比LightGBM与XGBoost的性能差异

## 创新点
1. 采用集成特征选择策略，提高特征选择的稳定性
2. 引入LightGBM作为补充模型，扩展模型多样性
3. 使用帕累托分析优化特征数量与性能的平衡
4. 实现模型性能的多维度评估和可视化
