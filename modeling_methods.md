# 机器学习建模方法详细说明

## 项目概述

本项目使用mlr3框架实现机器学习模型训练和评估，主要采用集成特征选择(Ensemble Feature Selection, EFS)方法进行特征筛选，并比较多种机器学习算法的性能。整个建模流程包括数据预处理、特征选择、模型训练、参数优化和性能评估等步骤。

## 1. 数据预处理

数据预处理步骤包括：

- 移除常量特征（使用`removeconstants`，阈值为0.05）
- 过滤低方差特征（使用`filter`，阈值为1e-9）
- 对连续变量进行缺失值处理（使用`imputehist`）
- 对分类变量进行缺失值处理（使用`imputemode`）

## 2. 数据划分

- 使用参数化的分割比例（2/3）将数据集分为训练集和测试集
- 可以使用预定义的分割文件（`split.map-group.txt`）
- 内部评估采用5折交叉验证

## 3. 集成特征选择(EFS)

EFS是本项目的核心方法，它结合多个学习器的特征重要性评估来获得一个稳定可靠的特征子集：

- 使用递归特征消除(RFE)作为基础特征选择器
- 设置初始特征数量为20，每次迭代移除15%的特征
- 结合四种不同学习器进行特征评估：
  - XGBoost (xgb)
  - 随机森林 (rf)
  - 支持向量机 (svm)
  - LightGBM (lgbm)

特征选择的评估指标：
- 内部重采样使用分类错误率(CE)
- 使用特征稳定性评估不同学习器之间的一致性（Jaccard相似度）
- 计算"knee points"找到特征数量和性能之间的平衡点

## 4. 模型训练与参数优化

对每个学习器执行自动参数调优：

### XGBoost参数空间
```
eta: 0.01-0.3
max_depth: 1-10
nrounds: 50-500
min_child_weight: 1-10
subsample: 0.5-1
colsample_bytree: 0.5-1
```

### 随机森林参数空间
```
num.trees: 500-1000
max.depth: 3-15
min.node.size: 1-10
mtry: 1-20
```

### SVM参数空间
```
cost: 10^(-2) - 10^2
kernel: polynomial, radial
degree: 1-3 (当kernel为polynomial时)
```

### LightGBM参数空间
```
learning_rate: 0.01-0.3
num_iterations: 50-500
max_depth: 3-12
min_data_in_leaf: 10-100
feature_fraction: 0.5-1.0
bagging_fraction: 0.5-1.0
bagging_freq: 0-10
min_gain_to_split: 0.0-0.5
num_leaves: 10-255
```

### Glmnet参数空间
```
lambda: 1e-4 - 1 (对数尺度)
alpha: 0-1
standardize: TRUE/FALSE
thresh: 1e-7 - 1e-4 (对数尺度)
maxit: 1000-10000
```

调优策略：
- 使用随机搜索算法(random_search)
- 批处理大小为100次评估
- 采用组合终止条件：
  1. 性能停滞（10次迭代内阈值0.005）
  2. 最大评估次数(100)
  3. 运行时间限制(1小时)
  4. 目标性能达成(0.85准确率)

## 5. 模型评估与比较

评估指标：
- 分类准确率(ACC)
- AUC曲线下面积
- 分类错误率(CE)

评估方法：
- 5折交叉验证(CV)
- 训练集内部评估
- 测试集外部验证

## 6. 特征重要性分析

针对不同模型提取特征重要性：
- 随机森林：使用impurity-based重要性
- XGBoost：使用Gain指标
- LightGBM：使用Gain指标
- Glmnet：使用系数绝对值
- SVM：使用支持向量系数

## 7. 可视化与结果输出

- ROC曲线比较(训练集、测试集、交叉验证)
- 特征重要性可视化
- 模型性能比较表格
- 集成特征选择(EFS)过程可视化：
  - 性能对比图
  - 特征数量对比图
  - 帕累托前沿图
  - 特征稳定性图

结果输出为Excel文件，包含：
- 各模型详细信息（性能指标、最优超参数、特征重要性）
- 模型间性能比较
- 重要特征解释

## 8. 实现细节

- 使用future包进行并行计算
- 加入模型封装和回退机制，提高鲁棒性
- 对特征进行重命名以适应算法要求
- 使用随机种子确保结果可重复性
- 所有模型都使用概率预测(`predict_type = "prob"`)，便于ROC分析
- 通过mlr3extralearners包扩展模型种类
- 使用封装在EFS方法中的回调函数，如one_se_rule提高特征选择效率
- 使用mlr3pipelines构建预处理流程

## 9. 结果解释

### 理解ROC曲线
ROC曲线(接收者操作特征曲线)展示了模型在不同分类阈值下的性能，通过绘制真阳性率(TPR)对假阳性率(FPR)的关系：
- 曲线越接近左上角，模型性能越好
- AUC值范围从0.5(相当于随机猜测)到1.0(完美分类)

### 解释特征重要性
特征重要性分数表示每个特征对模型预测的贡献：
- 分数越高，特征对预测的影响越大
- 不同模型的重要性计算方式不同，不可直接比较
- 重要性排名的稳定性比具体数值更具参考价值

### 理解参数影响
- 复杂度参数(如树深度、正则化参数)控制过拟合风险
- 学习率参数影响模型收敛速度和稳定性
- 随机性参数(如采样比例)增强模型鲁棒性

## 10. 参数选择建议

### XGBoost
- 对于小样本量，降低max_depth(3-5)，提高min_child_weight(>3)
- 调整subsample和colsample_bytree(0.6-0.9)以减少过拟合
- 使用较小的学习率(0.01-0.1)并增加迭代次数

### 随机森林
- num.trees通常设置为500+，确保足够的随机性
- mtry通常设为特征数量的平方根
- max.depth控制树的复杂度，设置为10-15通常足够

### SVM
- 线性核适用于高维数据
- 径向核(RBF)适用于非线性关系
- cost参数需要根据数据规模调整

### LightGBM
- num_leaves比max_depth更重要，控制模型复杂度
- min_data_in_leaf帮助减少过拟合
- 使用feature_fraction和bagging_fraction增强随机性